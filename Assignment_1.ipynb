{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Academic Intergrity Policy\n",
        "I certify that the code and data in this assignment were generated independently using only the tools and reosurces defined in the course and that i did not receive any external help coaching or contributions during the prodcution of this work.\n",
        "\n",
        "Name: Saikrishna Dirisala\n",
        "\n",
        "Date: 03/11/2022"
      ],
      "metadata": {
        "id": "iwMjmRaH-GYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the Required libraries"
      ],
      "metadata": {
        "id": "eyR0L3UIiiwg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkFiFj7Lia1m"
      },
      "outputs": [],
      "source": [
        "# Importing the matplotlib libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "from google.colab import widgets\n",
        "import time\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deterministic Environment"
      ],
      "metadata": {
        "id": "Jf3SvFmaipP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class deterministic_environment(gym.Env):\n",
        "  metadata = { 'render.modes': []}\n",
        "  def __init__(self) -> None:\n",
        "    self.states   = spaces.Discrete(25)\n",
        "    self.action_space  = spaces.Discrete(4)\n",
        "    self.reward = 0\n",
        "    self.reward_track = []\n",
        "    # All states in the grid\n",
        "    self.all_states_ = []\n",
        "    for i in range(-1,6):\n",
        "       for j in range(-1,6):\n",
        "          self.all_states_.append([i,j])\n",
        "\n",
        "    self.expected_states =[]\n",
        "    for i in range(0,5):\n",
        "       for j in range(0,5):\n",
        "          self.expected_states.append([i,j])\n",
        "\n",
        "    self.unexpexted_states = []\n",
        "    for element in self.all_states_:\n",
        "       if element in self.expected_states:\n",
        "         pass\n",
        "       else:\n",
        "        self.expected_states.append(element)\n",
        "\n",
        "  def step(self, action):\n",
        "    #Mentioning the actions\n",
        "    if action == 0:\n",
        "      self.agent_position[0] += 1       # action = MOVE DOWN\n",
        "    elif action == 1:\n",
        "      self.agent_position[0] -= 1       # action = MOVE UP\n",
        "    elif action == 2:\n",
        "      self.agent_position[1] += 1       # action = MOVE RIGHT\n",
        "    elif action == 3:\n",
        "      self.agent_position[1] -= 1       # action = MOVE LEFT\n",
        "    else:\n",
        "      print('Undefined action encountered in the process.....')\n",
        "      return 0\n",
        "\n",
        "    # Initiating the rewards to the agent.\n",
        "    self.present_reward = 0\n",
        "    if (np.array(self.agent_position) == np.array([0,2])).all():\n",
        "      self.present_reward = 1\n",
        "    if (np.array(self.agent_position) == np.array([0,4])).all():\n",
        "      self.present_reward = 3\n",
        "    if (np.array(self.agent_position) == np.array([1,2])).all():\n",
        "      self.present_reward = -10\n",
        "    if (np.array(self.agent_position) == np.array([1,4])).all():\n",
        "      self.present_reward = 3\n",
        "    if (np.array(self.agent_position) == np.array([2,1])).all():\n",
        "      self.present_reward = 1\n",
        "    if (np.array(self.agent_position) == np.array([2,3])).all():\n",
        "      self.present_reward = -10\n",
        "    if (np.array(self.agent_position) == np.array([3,1])).all():\n",
        "      self.present_reward = 1\n",
        "    if (np.array(self.agent_position) == np.array([3,2])).all():\n",
        "      self.present_reward = -10\n",
        "    if (np.array(self.agent_position) == np.array([3,4])).all():\n",
        "      self.present_reward = 1\n",
        "    if (np.array(self.agent_position) == np.array([4,1])).all():\n",
        "      self.present_reward = 3\n",
        "    if (np.array(self.agent_position) == np.array([4,3])).all():\n",
        "      self.present_reward = 10\n",
        "    if (np.array(self.agent_position) == np.array(self.goal_position)).all():\n",
        "      print('Goal state reached')\n",
        "      self.present_reward = 100\n",
        "    # initiating the negative rewards using the \n",
        "    \n",
        "    ''' Deleting the expected states form the unexpected states to obtain all unexpeted states '''\n",
        "    \n",
        "    #\n",
        "    if (np.array(self.agent_position) == np.array([-1,0])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([-1,1])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([-1,2])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([-1,3])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([-1,4])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([0,5])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([1,5])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([2,5])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([3,5])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([4,5])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([0,-1])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([1,-1])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([2,-1])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([3,-1])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([4,-1])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([5,0])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([5,1])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([5,2])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([5,3])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([5,4])).all():\n",
        "      self.present_reward = -1\n",
        "    \n",
        "\n",
        "    #updating the position of the agent.\n",
        "    self.agent_position = np.clip(self.agent_position, 0, 4)\n",
        "    self.state = np.zeros((5,5))\n",
        "    self.state[tuple(self.agent_position)] = 1  \n",
        "    self.state[tuple(self.goal_position)] = 0.5\n",
        "    observation = self.agent_position\n",
        "\n",
        "\n",
        "    # Exit Condition\n",
        "    done = True if (self.agent_position == self.goal_position).all() else False\n",
        "    self.info = {}\n",
        "   \n",
        "    return observation, self.present_reward, done, self.info\n",
        "\n",
        "  def reset(self):\n",
        "    #Resetting the everything like present state, action \n",
        "    self.present_reward = 0\n",
        "    self.agent_position = [0, 0]\n",
        "    self.goal_position  = [4, 4]\n",
        "    self.state = np.zeros((5, 5))\n",
        "    self.state[tuple(self.agent_position)] = 1\n",
        "    self.state[tuple(self.goal_position)]  = 0.5\n",
        "    return self.agent_position\n",
        "\n",
        "  def render(self):\n",
        "    plt.figure(figsize=(4,5))\n",
        "    plt.imshow(self.state)\n",
        "\n",
        "\n",
        "deter_env = deterministic_environment()"
      ],
      "metadata": {
        "id": "vFppF6CJihyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stochastic Environment"
      ],
      "metadata": {
        "id": "ATJYXMClQZMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Schotistic_environment(gym.Env):\n",
        "  metadata = { 'render.modes': []}\n",
        "  def __init__(self) -> None:\n",
        "    self.states   = spaces.Discrete(25)\n",
        "    self.action_space  = spaces.Discrete(4)\n",
        "    self.max_timelaps = 10\n",
        "    self.timestep = 0\n",
        "    self.reward = 0\n",
        "    self.reward_track = []\n",
        "\n",
        "  def step(self, action):\n",
        "\n",
        "    # observation = np.array([LEFT, RIGHT, UP, DOWN])\n",
        "    #Mentioning the actions\n",
        "    #print('Updating the action')\n",
        "     \n",
        "    self.observation = self.agent_position\n",
        "    \n",
        "    if action == 0:                     # action = MOVE DOWN\n",
        "      if np.random.uniform(0,1) > 0.01:\n",
        "        self.agent_position[0]  += 1 \n",
        "        \n",
        "      else:\n",
        "        self.agent_position[0]  -= 1 \n",
        "         \n",
        "   \n",
        "    elif action == 1:                   # action = MOVE UP\n",
        "      self.agent_position[0] -= 1 \n",
        "        \n",
        "\n",
        "    elif action == 2:                   # action = MOVE RIGHT\n",
        "      if np.random.uniform(0,1) > 0.01:\n",
        "        self.agent_position[1] += 1 \n",
        "        \n",
        "      else:\n",
        "        self.agent_position[1] -= 1\n",
        "              \n",
        "    \n",
        "    elif action == 3:                   # action = MOVE LEFT\n",
        "      self.agent_position[1] -= 1        \n",
        "    \n",
        "    else:\n",
        "      #print('Undefined action encountered in the process.....')\n",
        "      return 0\n",
        "    \n",
        "  #updating the position of the agent.\n",
        "    self.agent_position = np.clip(self.agent_position, 0, 4)\n",
        "    self.state = np.zeros((5,5))\n",
        "    self.state[tuple(self.agent_position)] = 1  \n",
        "    self.state[tuple(self.goal_position)] = 0.5\n",
        "    observation = self.agent_position\n",
        "\n",
        "    # Initiating the rewards to the agent.\n",
        "    self.present_reward = 0\n",
        "    if (np.array(self.agent_position) == np.array([0,2])).all():\n",
        "      self.present_reward = 1\n",
        "    if (np.array(self.agent_position) == np.array([0,4])).all():\n",
        "      self.present_reward = 3\n",
        "    if (np.array(self.agent_position) == np.array([1,2])).all():\n",
        "      self.present_reward = -10\n",
        "    if (np.array(self.agent_position) == np.array([1,4])).all():\n",
        "      self.present_reward = 3\n",
        "    if (np.array(self.agent_position) == np.array([2,1])).all():\n",
        "      self.present_reward = 1\n",
        "    if (np.array(self.agent_position) == np.array([2,3])).all():\n",
        "      self.present_reward = -10\n",
        "    if (np.array(self.agent_position) == np.array([3,1])).all():\n",
        "      self.present_reward = 1\n",
        "    if (np.array(self.agent_position) == np.array([3,2])).all():\n",
        "      self.present_reward = -10\n",
        "    if (np.array(self.agent_position) == np.array([3,4])).all():\n",
        "      self.present_reward = 1\n",
        "    if (np.array(self.agent_position) == np.array([4,1])).all():\n",
        "      self.present_reward = 3\n",
        "    if (np.array(self.agent_position) == np.array([4,3])).all():\n",
        "      self.present_reward = 10\n",
        "    if (np.array(self.agent_position) == np.array(self.goal_position)).all():\n",
        "      print('Goal state reached')\n",
        "      self.present_reward = 100\n",
        "    # initiating the negative rewards using the \n",
        "    \n",
        "    ''' Deleting the expected states form the unexpected states to obtain all unexpeted states '''\n",
        "    \n",
        "    #\n",
        "    if (np.array(self.agent_position) == np.array([-1,0])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([-1,1])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([-1,2])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([-1,3])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([-1,4])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([0,5])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([1,5])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([2,5])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([3,5])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([4,5])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([0,-1])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([1,-1])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([2,-1])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([3,-1])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([4,-1])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([5,0])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([5,1])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([5,2])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([5,3])).all():\n",
        "      self.present_reward = -1\n",
        "    if (np.array(self.agent_position) == np.array([5,4])).all():\n",
        "      self.present_reward = -1\n",
        "    \n",
        "\n",
        "    #updating the position of the agent.\n",
        "    self.agent_position = np.clip(self.agent_position, 0, 4)\n",
        "    self.state = np.zeros((5,5))\n",
        "    self.state[tuple(self.agent_position)] = 1  \n",
        "    self.state[tuple(self.goal_position)] = 0.5\n",
        "    observation = self.agent_position\n",
        "\n",
        "\n",
        "    # Exit Condition\n",
        "    done = True if (self.agent_position == self.goal_position).all() else False\n",
        "    self.info = {}\n",
        "   \n",
        "    return observation, self.present_reward, done, self.info\n",
        "\n",
        "  def reset(self):\n",
        "    #Resetting the everything like present state, action \n",
        "    self.present_reward = 0\n",
        "    self.agent_position = [0, 0]\n",
        "    self.goal_position  = [4, 4]\n",
        "    self.state = np.zeros((5, 5))\n",
        "    self.state[tuple(self.agent_position)] = 1\n",
        "    self.state[tuple(self.goal_position)]  = 0.5\n",
        "    return self.agent_position\n",
        "\n",
        "  def render(self):\n",
        "    plt.figure(figsize=(4,5))\n",
        "    plt.xlabel('The Grid Environment of size 5x5')\n",
        "    plt.imshow(self.state)\n",
        "\n",
        "env_scho = Schotistic_environment()"
      ],
      "metadata": {
        "id": "Y-aj-yKcQYll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning On deterministic Environment"
      ],
      "metadata": {
        "id": "St3rWxGTizJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearning:\n",
        "    \"\"\"This class implements the Q-learning algorithm.\"\"\"\n",
        "\n",
        "    def __init__(self, input_environment):\n",
        "        \"\"\"This method instantiates the Q-learning parameters.\"\"\"\n",
        "\n",
        "        self.environment = input_environment\n",
        "\n",
        "        \"\"\"TO DO: Instantiate the Q-learning parameters.\"\"\"\n",
        "        # Initializing the actions and states\n",
        "        self.actions_     = 4\n",
        "        self.observation_ = 4\n",
        "        self.learning_rate = 0.15\n",
        "        self.gamma        = 0.2\n",
        "        self.epsilon      = 1.0\n",
        "\n",
        "        #Declaring the Q-Matrix\n",
        "\n",
        "        self.Q_matrix = np.zeros((5,5,4))\n",
        "\n",
        "        # for i in range(5):\n",
        "        #   for j in range(5):\n",
        "        #     for k in range(4):\n",
        "        #       self.Q_matrix[i,j,k] = random.uniform(0,1)\n",
        "        # for i in range(4):\n",
        "        #   self.Q_matrix[4,4,i] = 0\n",
        "        \n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"This method performs the agent training.\"\"\"\n",
        "        rewards_ = []\n",
        "        epsilon_ = []\n",
        "  \n",
        "        # Declaring the no of epochs\n",
        "        output_grid = widgets.Grid(1, 1)\n",
        "\n",
        "        for i in range(1000):\n",
        "          print(i)\n",
        "          initial_observation = self.environment.reset()\n",
        "          # Cummulative reward storage\n",
        "          cummulative_reward = 0\n",
        "\n",
        "          while 1:\n",
        "            if self.epsilon <0.5: # Implementing the Exploitaiton\n",
        "              action = np.argmax(self.Q_matrix[initial_observation[0],initial_observation[1]])\n",
        "            else:                # Implementing the Exploration\n",
        "              action = np.random.randint(0,self.environment.action_space.n)\n",
        "            # Observing the next state using the step method in environment\n",
        "            new_observation, reward, done, info = self.environment.step(action)\n",
        "            cummulative_reward += reward\n",
        "            \n",
        "            # Updating the Q_table \n",
        "            new_observation = np.clip(new_observation, 0, 4)\n",
        "           \n",
        "            initial_observation = np.clip(initial_observation,0,4)\n",
        "            self.Q_matrix[initial_observation[0],initial_observation[1],action] = (self.Q_matrix[initial_observation[0],initial_observation[1],action])+self.learning_rate*(reward + self.gamma * np.max(self.Q_matrix[new_observation[0],new_observation[1]])-(self.Q_matrix[initial_observation[0],initial_observation[1],action]))\n",
        "            \n",
        "            # Updating the observation\n",
        "            initial_observation = new_observation\n",
        "            # Breaking the loop \n",
        "\n",
        "            if done:\n",
        "                #print('The goal state reached..')\n",
        "                break\n",
        "\n",
        "          # Updating the Epsilon value\n",
        "          epsilon_.append(self.epsilon)\n",
        "          self.epsilon = self.epsilon*0.99\n",
        "          rewards_.append(cummulative_reward)\n",
        "          \n",
        "       \n",
        "        return np.array(epsilon_),np.array(rewards_)\n",
        "               \n",
        "          \n",
        "    def evaluate(self):\n",
        "        \"\"\"This method evaluate the trained agent's performance.\"\"\"\n",
        "\n",
        "        \"\"\"\"TO DO: Evaluate the trained agent's performance by selecting only the greedy/best action in each state.\"\"\"\n",
        "        self.environment.train = False\n",
        "        iterations = 30\n",
        "        observation = self.environment.reset()\n",
        "        rewards_ = []\n",
        "        for i in range(0,iterations):\n",
        "          print(i)\n",
        "          initial_observation = self.environment.reset()\n",
        "\n",
        "          # Cummulative reward storage\n",
        "          cummulative_reward = 0\n",
        "\n",
        "          while 1:        \n",
        "            action = np.argmax(self.Q_matrix[initial_observation[0],initial_observation[1]])\n",
        "    \n",
        "            # Observing the next state using the step method in environment\n",
        "            new_observation, reward, done, info = self.environment.step(action)\n",
        "            \n",
        "            cummulative_reward += reward\n",
        "\n",
        "            # with output_grid.output_to(0, 0):\n",
        "            #   output_grid.clear_cell()\n",
        "            #   deter_env.render()\n",
        "            # time.sleep(1)        \n",
        "  \n",
        "            new_observation = np.clip(new_observation, 0, 4)  \n",
        "            # Updating the observation\n",
        "            initial_observation = new_observation\n",
        "            if done:\n",
        "               \n",
        "                break\n",
        "          rewards_.append(cummulative_reward)\n",
        "        return rewards_\n",
        "\n",
        "Q_Object = QLearning(deter_env)"
      ],
      "metadata": {
        "id": "Ho6ljx64i2NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "=>Training With Q_learning Deterministic Environment"
      ],
      "metadata": {
        "id": "0K_sDMb2i7HT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(epsilon_values,rewards_values) =Q_Object.train()"
      ],
      "metadata": {
        "id": "COc15fL4i6nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Plotting the CUMMULATIVE REWARDS and EPSILON graphs for Q-Learning"
      ],
      "metadata": {
        "id": "kmW6puyYjEPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "\n",
        "# Cummulative Reward Graph\n",
        "plt.plot(range(0,len(rewards_values)), rewards_values)\n",
        "matplotlib.pyplot.title(' Deterministic Training Graph',size=18, loc= 'Center',pad =20)\n",
        "plt.xlabel('No of Epochs')\n",
        "plt.ylabel('The Rewards Obtained')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Epsilon Graph\n",
        "matplotlib.pyplot.title(' Deterministic Training Graph',size=18, loc= 'Center',pad =20)\n",
        "plt.xlabel('No of Epochs')\n",
        "plt.ylabel('Epsilon Value')\n",
        "plt.plot(range(0,len(epsilon_values)),epsilon_values)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YOkDOunjjJfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: - During the training process in Q-Learning, the learning rate of the model should fix greater or eqaual to 0.15. Otherwise, the agent will fall under a loop."
      ],
      "metadata": {
        "id": "RjWWpchEkekc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Evaluating Process with Q-Learning Deterministic environment"
      ],
      "metadata": {
        "id": "jrHjPHCplqM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rewards_values_det = Q_Object.evaluate()"
      ],
      "metadata": {
        "id": "zN84GzxBmXnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Cummulative rewards obtained during Evaluation by Q_learning deterministic environmnet"
      ],
      "metadata": {
        "id": "rAGpL3GhlPkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cummulative rewards obtained during Evaluation\n",
        "matplotlib.pyplot.title(' Deterministic Q-Learning Evaluation Graph',size=18, loc= 'Center',pad =20)\n",
        "plt.plot(range(0,len(rewards_values_det)), rewards_values_det)\n",
        "plt.xlabel('No of Epochs')\n",
        "plt.ylabel('The Rewards Obtained')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IqWxBRlel77B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning on Stochastic Environment"
      ],
      "metadata": {
        "id": "jqbRrofvSor3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearning:\n",
        "    \"\"\"This class implements the Q-learning algorithm.\"\"\"\n",
        "\n",
        "    def __init__(self, input_environment):\n",
        "        \"\"\"This method instantiates the Q-learning parameters.\"\"\"\n",
        "\n",
        "        self.environment = input_environment\n",
        "\n",
        "        \"\"\"TO DO: Instantiate the Q-learning parameters.\"\"\"\n",
        "        # Initializing the actions and states\n",
        "        self.actions_     = 4\n",
        "        self.observation_ = 4\n",
        "        self.learning_rate = 0.2\n",
        "        self.gamma        = 0.9\n",
        "        self.epsilon      = 1.0\n",
        "\n",
        "        #Declaring the Q-Matrix\n",
        "\n",
        "        self.Q_matrix = np.zeros((5,5,4))\n",
        "\n",
        "        for i in range(5):\n",
        "          for j in range(5):\n",
        "            for k in range(4):\n",
        "              self.Q_matrix[i,j,k] = random.uniform(0,1)\n",
        "        for i in range(4):\n",
        "          self.Q_matrix[4,4,i] = 0\n",
        "        \n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"This method performs the agent training.\"\"\"\n",
        "        rewards_ = []\n",
        "        epsilon_ = []\n",
        "  \n",
        "        # Declaring the no of epochs\n",
        "        output_grid = widgets.Grid(1, 1)\n",
        "\n",
        "        for i in range(5000):\n",
        "          print(i)\n",
        "          initial_observation = self.environment.reset()\n",
        "          # Cummulative reward storage\n",
        "          cummulative_reward = 0\n",
        "\n",
        "          while 1:\n",
        "            if self.epsilon <0.6: # Implementing the Exploitaiton\n",
        "              action = np.argmax(self.Q_matrix[initial_observation[0],initial_observation[1]])\n",
        "            else:                # Implementing the Exploration\n",
        "              action = np.random.randint(0,self.environment.action_space.n)\n",
        "            # Observing the next state using the step method in environment\n",
        "            new_observation, reward, done, info = self.environment.step(action)\n",
        "            cummulative_reward += reward\n",
        "           \n",
        "            # Updating the Q_table \n",
        "            new_observation = np.clip(new_observation, 0, 4)\n",
        "           \n",
        "            initial_observation = np.clip(initial_observation,0,4)\n",
        "            self.Q_matrix[initial_observation[0],initial_observation[1],action] = (self.Q_matrix[initial_observation[0],initial_observation[1],action])+self.learning_rate*(reward + self.gamma * np.max(self.Q_matrix[new_observation[0],new_observation[1]])-(self.Q_matrix[initial_observation[0],initial_observation[1],action]))\n",
        "            \n",
        "            # Updating the observation\n",
        "            initial_observation = new_observation\n",
        "            # Breaking the loop \n",
        "\n",
        "            if done:\n",
        "                #print('The goal state reached..')\n",
        "                break\n",
        "\n",
        "          # Updating the Epsilon value\n",
        "          epsilon_.append(self.epsilon)\n",
        "          self.epsilon = self.epsilon*0.99\n",
        "          rewards_.append(cummulative_reward)\n",
        "          \n",
        "       \n",
        "        return np.array(epsilon_),np.array(rewards_)\n",
        "               \n",
        "          \n",
        "    def evaluate(self):\n",
        "        \"\"\"This method evaluate the trained agent's performance.\"\"\"\n",
        "\n",
        "        \"\"\"\"TO DO: Evaluate the trained agent's performance by selecting only the greedy/best action in each state.\"\"\"\n",
        "        self.environment.train = False\n",
        "        iterations = 90\n",
        "        observation = self.environment.reset()\n",
        "        rewards_ = []\n",
        "        for i in range(0,iterations):\n",
        "          print(i)\n",
        "          initial_observation = self.environment.reset()\n",
        "\n",
        "          # Cummulative reward storage\n",
        "          cummulative_reward = 0\n",
        "\n",
        "          while 1:        \n",
        "            action = np.argmax(self.Q_matrix[initial_observation[0],initial_observation[1]])\n",
        "    \n",
        "            # Observing the next state using the step method in environment\n",
        "            new_observation, reward, done, info = self.environment.step(action)\n",
        "            \n",
        "            cummulative_reward += reward\n",
        "\n",
        "            # with output_grid.output_to(0, 0):\n",
        "            #   output_grid.clear_cell()\n",
        "            #   deter_env.render()\n",
        "            # time.sleep(1)        \n",
        "  \n",
        "            new_observation = np.clip(new_observation, 0, 4)  \n",
        "            # Updating the observation\n",
        "            initial_observation = new_observation\n",
        "            if done:\n",
        "               \n",
        "                break\n",
        "          rewards_.append(cummulative_reward)\n",
        "        return rewards_\n",
        "\n",
        "Q_Object_scho = QLearning(env_scho)"
      ],
      "metadata": {
        "id": "PgyicfxASvhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Training the data using Q_learning Stochastic Environment"
      ],
      "metadata": {
        "id": "kXU5gEUslE67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(epsilon_values_sto,rewards_values_sto) = Q_Object_scho.train()"
      ],
      "metadata": {
        "id": "JdlyGoApTBA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Showing the graphs from training using Q-Learning Stochastic environment"
      ],
      "metadata": {
        "id": "TFPwkpxPk3Yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "\n",
        "# Cummulative Reward Graph\n",
        "plt.plot(range(0,len(rewards_values_sto)), rewards_values_sto)\n",
        "matplotlib.pyplot.title(' Stochastic Training Graph',size=18, loc= 'Center',pad =20)\n",
        "plt.xlabel('No of Epochs')\n",
        "plt.ylabel('The Rewards Obtained')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Epsilon Graph\n",
        "matplotlib.pyplot.title(' Stochastic Training Graph',size=18, loc= 'Center',pad =20)\n",
        "plt.xlabel('No of Epochs')\n",
        "plt.ylabel('Epsilon Value')\n",
        "plt.plot(range(0,len(epsilon_values_sto)),epsilon_values_sto)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eBa7AImyTxjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Evaluating the Process using stochastic environment"
      ],
      "metadata": {
        "id": "m50BiOnjU_19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rewards_values_sto = Q_Object_scho.evaluate()"
      ],
      "metadata": {
        "id": "zDH2Ph6tU_HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Rewards obtained by using Q-Learning from stochastic Environment"
      ],
      "metadata": {
        "id": "ro42En3ikqs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cummulative rewards obtained during Evaluation\n",
        "\n",
        "matplotlib.pyplot.title(' Stochastic Q-Learning Evaluation Graph',size=18, loc= 'Center',pad =20)\n",
        "plt.plot(range(0,len(rewards_values_sto)), rewards_values_sto)\n",
        "plt.xlabel('No of Epochs')\n",
        "plt.ylabel('The Rewards Obtained')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8KhdRgkxWSLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SARSA on Deterministic Environment"
      ],
      "metadata": {
        "id": "-MehbIbolL2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SARSA_:\n",
        "    \"\"\"This class implements the Q-learning algorithm.\"\"\"\n",
        "\n",
        "    def __init__(self, input_environment):\n",
        "        \"\"\"This method instantiates the Q-learning parameters.\"\"\"\n",
        "\n",
        "        self.environment = input_environment\n",
        "\n",
        "        \"\"\"TO DO: Instantiate the Q-learning parameters.\"\"\"\n",
        "        # Initializing the actions and states\n",
        "        self.actions_     = 4\n",
        "        self.observation_ = 4\n",
        "        self.learning_rate = 0.2\n",
        "        self.gamma        = 0.8\n",
        "        self.epsilon      = 1.0\n",
        "\n",
        "        #Declaring the Q-Matrix\n",
        "\n",
        "        self.Q_matrix = np.zeros((5,5,4))\n",
        "\n",
        "        # for i in range(5):\n",
        "        #   for j in range(5):\n",
        "        #     for k in range(4):\n",
        "        #       self.Q_matrix[i,j,k] = random.uniform(0,1)\n",
        "        # for i in range(4):\n",
        "        #   self.Q_matrix[4,4,i] = 0\n",
        "        \n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"This method performs the agent training.\"\"\"\n",
        "        rewards_ = []\n",
        "        epsilon_ = []\n",
        "  \n",
        "        # Declaring the no of epochs\n",
        "        output_grid = widgets.Grid(1, 1)\n",
        "\n",
        "        for i in range(10000):\n",
        "          print(i)\n",
        "          initial_observation = self.environment.reset()\n",
        "          # Cummulative reward storage\n",
        "          cummulative_reward = 0\n",
        "          \n",
        "          # Observation values\n",
        "          observation_value_1 = 0\n",
        "          observation_value_2 = 0\n",
        "          observation_value_3 = 0\n",
        "\n",
        "          while 1:\n",
        "            if self.epsilon <0.9: # Implementing the Exploitaiton\n",
        "              action = np.argmax(self.Q_matrix[initial_observation[0],initial_observation[1]])\n",
        "            else:                # Implementing the Exploration\n",
        "              action = np.random.randint(0,self.environment.action_space.n)\n",
        "            # Observing the next state using the step method in environment\n",
        "            new_observation, reward, done, info = self.environment.step(action)\n",
        "            cummulative_reward += reward\n",
        "\n",
        "            # with output_grid.output_to(0, 0):\n",
        "            #   output_grid.clear_cell()\n",
        "            #   deter_env.render()\n",
        "            # time.sleep(1)        \n",
        "            \n",
        "            # Updating the Q_table \n",
        "            observation_value_1 = new_observation\n",
        "            observation_value_2 = observation_value_1\n",
        "            observation_value_3 = observation_value_2\n",
        "            if (np.array(observation_value_1) == np.array(observation_value_2)).all() and (np.array(observation_value_2) == np.array(observation_value_3)).all():\n",
        "              #print(\"loop detected\")\n",
        "              action = np.random.randint(0,self.environment.action_space.n)\n",
        "            if (np.array(observation_value_1) == np.array(observation_value_3)).all():\n",
        "              #print(\"Might have chances to fall in loop or mistracking\")\n",
        "              action = np.random.randint(0,self.environment.action_space.n)\n",
        "            \n",
        "\n",
        "\n",
        "            new_observation = np.clip(new_observation, 0, 4)\n",
        "            if self.epsilon <0.5: # Implementing the Exploitaiton\n",
        "              action_new = np.argmax(self.Q_matrix[new_observation[0],new_observation[1]])\n",
        "            else:                # Implementing the Exploration\n",
        "              action_new = np.random.randint(0,self.environment.action_space.n)\n",
        "            \n",
        "            initial_observation = np.clip(initial_observation,0,4)\n",
        "\n",
        "            # Writing the condition to check the agent falls in the loop or not\n",
        "            \n",
        "            self.Q_matrix[initial_observation[0],initial_observation[1],action] = (self.Q_matrix[initial_observation[0],initial_observation[1],action])+self.learning_rate*(reward + self.gamma * (self.Q_matrix[new_observation[0],new_observation[1],action_new])-(self.Q_matrix[initial_observation[0],initial_observation[1],action]))\n",
        "            \n",
        "            # Updating the observation\n",
        "            initial_observation = new_observation\n",
        "            # Breaking the loop \n",
        "\n",
        "            if done:\n",
        "                #print('The goal state reached..')\n",
        "                break\n",
        "\n",
        "          # Updating the Epsilon value\n",
        "          epsilon_.append(self.epsilon)\n",
        "          self.epsilon = self.epsilon*0.99\n",
        "          rewards_.append(cummulative_reward)\n",
        "          \n",
        "       \n",
        "        return np.array(epsilon_),np.array(rewards_)\n",
        "               \n",
        "          \n",
        "    def evaluate(self):\n",
        "        \"\"\"This method evaluate the trained agent's performance.\"\"\"\n",
        "\n",
        "        \"\"\"\"TO DO: Evaluate the trained agent's performance by selecting only the greedy/best action in each state.\"\"\"\n",
        "        self.environment.train = False\n",
        "        iterations = 30\n",
        "        observation = self.environment.reset()\n",
        "        rewards_ = []\n",
        "        for i in range(0,iterations):\n",
        "          print(i)\n",
        "          initial_observation = self.environment.reset()\n",
        "\n",
        "          # Cummulative reward storage\n",
        "          cummulative_reward = 0\n",
        "\n",
        "          while 1:        \n",
        "            action = np.argmax(self.Q_matrix[initial_observation[0],initial_observation[1]])\n",
        "    \n",
        "            # Observing the next state using the step method in environment\n",
        "            new_observation, reward, done, info = self.environment.step(action)\n",
        "            \n",
        "            cummulative_reward += reward\n",
        "\n",
        "            # with output_grid.output_to(0, 0):\n",
        "            #   output_grid.clear_cell()\n",
        "            #   deter_env.render()\n",
        "            # time.sleep(1)        \n",
        "  \n",
        "            new_observation = np.clip(new_observation, 0, 4)  \n",
        "            # Updating the observation\n",
        "            initial_observation = new_observation\n",
        "            if done:\n",
        "               \n",
        "                break\n",
        "          rewards_.append(cummulative_reward)\n",
        "        return rewards_\n",
        "\n",
        "\n",
        "S_Object = SARSA_(deter_env)"
      ],
      "metadata": {
        "id": "s2NdTBIOlRYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q_matrix[initial_observation,action] = (Q_matrix[initial_observation,action])+learning_rate*(reward +gamma * (Q_matrix[new_observation,action_new])-(Q_matrix[initial_observation,action]))\n",
        "            "
      ],
      "metadata": {
        "id": "wsMPtquB5Qq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Training the process of SARASA with deterministic Environment"
      ],
      "metadata": {
        "id": "yk3bGv7EnhId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(epsilon_values_1,reward_values_1) = S_Object.train()"
      ],
      "metadata": {
        "id": "9dDD1NKnnQtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Plotting the Cummulative Reward and Epsilon using SARSA"
      ],
      "metadata": {
        "id": "ijJBrtVSKYvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "\n",
        "# Plotting the Cummulative Reward\n",
        "\n",
        "plt.plot(range(0,len(reward_values_1)), reward_values_1)\n",
        "matplotlib.pyplot.title(' Deterministic SARSA Training Graph',size=18, loc= 'Center',pad =20)\n",
        "plt.xlabel('No of Epochs')\n",
        "plt.ylabel('The Rewards Obtained')\n",
        "plt.show()\n",
        "\n",
        "# Epsilon Graph\n",
        "matplotlib.pyplot.title(' Deterministic SARSA Training Graph',size=18, loc= 'Center',pad =20)\n",
        "plt.xlabel('No of Epochs')\n",
        "plt.ylabel('Epsilon Value')\n",
        "plt.plot(range(0,len(epsilon_values_1)),epsilon_values_1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GGpExvusnvkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Evalution process of Deterministic environment by SARSA\n"
      ],
      "metadata": {
        "id": "avgmxxsnXsFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rewards_values_SARSA = S_Object.evaluate()"
      ],
      "metadata": {
        "id": "6NdxB_TZn0F9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Rewards obtained from the Evaluation stage"
      ],
      "metadata": {
        "id": "6ijHuSOIkU09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cummulative rewards obtained during Evaluation\n",
        "\n",
        "matplotlib.pyplot.title(' Deterministic SARSA Evaluation Graph',size=18, loc= 'Center',pad =20)\n",
        "plt.plot(range(0,len(rewards_values_SARSA)), rewards_values_SARSA)\n",
        "plt.xlabel('No of Epochs')\n",
        "plt.ylabel('The Rewards Obtained')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3x24NpVeXzUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SARSA on Stochastic Environment"
      ],
      "metadata": {
        "id": "Ayq7MSN7Y2q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SARSA_:\n",
        "    \"\"\"This class implements the Q-learning algorithm.\"\"\"\n",
        "\n",
        "    def __init__(self, input_environment):\n",
        "        \"\"\"This method instantiates the Q-learning parameters.\"\"\"\n",
        "\n",
        "        self.environment = input_environment\n",
        "\n",
        "        \"\"\"TO DO: Instantiate the Q-learning parameters.\"\"\"\n",
        "        # Initializing the actions and states\n",
        "        self.actions_     = 4\n",
        "        self.observation_ = 4\n",
        "        self.learning_rate = 0.8\n",
        "        self.gamma        = 0.9\n",
        "        self.epsilon      = 1.0\n",
        "\n",
        "        #Declaring the Q-Matrix\n",
        "\n",
        "        self.Q_matrix = np.zeros((5,5,4))\n",
        "\n",
        "        # for i in range(5):\n",
        "        #   for j in range(5):\n",
        "        #     for k in range(4):\n",
        "        #       self.Q_matrix[i,j,k] = random.uniform(0,1)\n",
        "        # for i in range(4):\n",
        "        #   self.Q_matrix[4,4,i] = 0\n",
        "        \n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"This method performs the agent training.\"\"\"\n",
        "        rewards_ = []\n",
        "        epsilon_ = []\n",
        "  \n",
        "        # Declaring the no of epochs\n",
        "        output_grid = widgets.Grid(1, 1)\n",
        "\n",
        "        for i in range(1000):\n",
        "          print(i)\n",
        "          initial_observation = self.environment.reset()\n",
        "          # Cummulative reward storage\n",
        "          cummulative_reward = 0\n",
        "          \n",
        "          # Observation values\n",
        "          observation_value_1 = 0\n",
        "          observation_value_2 = 0\n",
        "          observation_value_3 = 0\n",
        "\n",
        "          while 1:\n",
        "            if self.epsilon <0.7: # Implementing the Exploitaiton\n",
        "              action = np.argmax(self.Q_matrix[initial_observation[0],initial_observation[1]])\n",
        "            else:                # Implementing the Exploration\n",
        "              action = np.random.randint(0,self.environment.action_space.n)\n",
        "            # Observing the next state using the step method in environment\n",
        "            new_observation, reward, done, info = self.environment.step(action)\n",
        "            cummulative_reward += reward\n",
        "\n",
        "            # with output_grid.output_to(0, 0):\n",
        "            #   output_grid.clear_cell()\n",
        "            #   deter_env.render()\n",
        "            # time.sleep(1)        \n",
        "            \n",
        "            #Updating the Q_table \n",
        "            observation_value_1 = new_observation\n",
        "            observation_value_2 = observation_value_1\n",
        "            observation_value_3 = observation_value_2\n",
        "            if (np.array(observation_value_1) == np.array(observation_value_2)).all() and (np.array(observation_value_2) == np.array(observation_value_3)).all():\n",
        "              #print(\"loop detected\")\n",
        "              action = np.random.randint(0,self.environment.action_space.n)\n",
        "            if (np.array(observation_value_1) == np.array(observation_value_3)).all():\n",
        "              #print(\"Might have chances to fall in loop or mistracking\")\n",
        "              action = np.random.randint(0,self.environment.action_space.n)\n",
        "            \n",
        "\n",
        "\n",
        "            new_observation = np.clip(new_observation, 0, 4)\n",
        "            if self.epsilon <0.7: # Implementing the Exploitaiton\n",
        "              action_new = np.argmax(self.Q_matrix[new_observation[0],new_observation[1]])\n",
        "            else:                # Implementing the Exploration\n",
        "              action_new = np.random.randint(0,self.environment.action_space.n)\n",
        "            \n",
        "            initial_observation = np.clip(initial_observation,0,4)\n",
        "\n",
        "            # Writing the condition to check the agent falls in the loop or not\n",
        "            \n",
        "            self.Q_matrix[initial_observation[0],initial_observation[1],action] = (self.Q_matrix[initial_observation[0],initial_observation[1],action])+self.learning_rate*(reward + self.gamma * (self.Q_matrix[new_observation[0],new_observation[1],action_new])-(self.Q_matrix[initial_observation[0],initial_observation[1],action]))\n",
        "            \n",
        "            # Updating the observation\n",
        "            initial_observation = new_observation\n",
        "            # Breaking the loop \n",
        "\n",
        "            if done:\n",
        "                #print('The goal state reached..')\n",
        "                break\n",
        "\n",
        "          # Updating the Epsilon value\n",
        "          epsilon_.append(self.epsilon)\n",
        "          self.epsilon = self.epsilon*0.99\n",
        "          rewards_.append(cummulative_reward)\n",
        "          \n",
        "       \n",
        "        return np.array(epsilon_),np.array(rewards_)\n",
        "               \n",
        "          \n",
        "    def evaluate(self):\n",
        "        \"\"\"This method evaluate the trained agent's performance.\"\"\"\n",
        "\n",
        "        \"\"\"\"TO DO: Evaluate the trained agent's performance by selecting only the greedy/best action in each state.\"\"\"\n",
        "        self.environment.train = False\n",
        "        iterations = 30\n",
        "        observation = self.environment.reset()\n",
        "        rewards_ = []\n",
        "        for i in range(0,iterations):\n",
        "          print(i)\n",
        "          initial_observation = self.environment.reset()\n",
        "\n",
        "          # Cummulative reward storage\n",
        "          cummulative_reward = 0\n",
        "          while 1:        \n",
        "            action = np.argmax(self.Q_matrix[initial_observation[0],initial_observation[1]])\n",
        "    \n",
        "            # Observing the next state using the step method in environment\n",
        "            new_observation, reward, done, info = self.environment.step(action)\n",
        "            \n",
        "            cummulative_reward += reward\n",
        "\n",
        "            new_observation = np.clip(new_observation, 0, 4)  \n",
        "            print(new_observation)\n",
        "            # Updating the observation\n",
        "            initial_observation = new_observation\n",
        "            if done:\n",
        "               \n",
        "                break\n",
        "          rewards_.append(cummulative_reward)\n",
        "        return rewards_\n",
        "\n",
        "\n",
        "SARSA_Object_STOCHASTIC = SARSA_(env_scho)"
      ],
      "metadata": {
        "id": "TiYRfiACY8vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(epsilon_values_sto_SARSA,reward_values_sto_SARSA) = SARSA_Object_STOCHASTIC.train()"
      ],
      "metadata": {
        "id": "0Ws8-95GZDoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Showing the Graphs of Epsilon and rewards from train data by Stochastic environment using SARSA"
      ],
      "metadata": {
        "id": "bsXxz9-DjQOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "\n",
        "# Plotting the Cummulative Reward\n",
        "\n",
        "plt.plot(range(0,len(reward_values_sto_SARSA)), reward_values_sto_SARSA)\n",
        "matplotlib.pyplot.title(' Deterministic Training Graph',size=18, loc= 'Center',pad =20)\n",
        "plt.xlabel('No of Epochs')\n",
        "plt.ylabel('The Rewards Obtained')\n",
        "plt.show()\n",
        "\n",
        "# Epsilon Graph\n",
        "matplotlib.pyplot.title(' Deterministic Training Graph',size=18, loc= 'Center',pad =20)\n",
        "plt.xlabel('No of Epochs')\n",
        "plt.ylabel('Epsilon Value')\n",
        "plt.plot(range(0,len(epsilon_values_sto_SARSA)),epsilon_values_sto_SARSA)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "18Y2lz8YZEF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Evaluate rewards for Stochastic Environment in SARSA"
      ],
      "metadata": {
        "id": "PP8JjY3njJHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rewards_values_SARSA_Evaluate = SARSA_Object_STOCHASTIC.evaluate()"
      ],
      "metadata": {
        "id": "-g49-dX6ibfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Showing the Graph of Rewards obtained in Evaluation from stochastic Environment SARSA"
      ],
      "metadata": {
        "id": "PwN4Jvi_jb-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cummulative rewards obtained during Evaluation\n",
        "plt.plot(range(0,len(rewards_values_SARSA_Evaluate)), rewards_values_SARSA_Evaluate)\n",
        "plt.xlabel('No of Epochs')\n",
        "plt.ylabel('The Rewards Obtained')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "33ds8efXbLOD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}